(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{426:function(t,a,s){"use strict";s.r(a);var e=s(11),i=Object(e.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"_1-gemm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-gemm"}},[t._v("#")]),t._v(" 1. GEMM")]),t._v(" "),a("p",[t._v("BLAS (Basic Linear Algebra Subprograms) 是线性代数接口的规范。")]),t._v(" "),a("p",[t._v("GEMM（General Matrix to Matrix Multiplication，通用矩阵乘）是 BLAS 的一部分，核心就是将两个矩阵相乘。更准确地，是给定矩阵 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("A")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mi",[t._v("B")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mi",[t._v("C")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A, B, C")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.8777699999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit"},[t._v("A")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")])])])]),t._v(" 和数字 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("α")]),a("mo",{attrs:{separator:"true"}},[t._v(",")]),a("mi",[t._v("β")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\alpha, \\beta")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mpunct"},[t._v(",")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")])])])]),t._v("，计算：")]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("C")]),a("mo",[t._v("←")]),a("mi",[t._v("α")]),a("mi",[t._v("A")]),a("mi",[t._v("B")]),a("mo",[t._v("+")]),a("mi",[t._v("β")]),a("mi",[t._v("C")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("C \\leftarrow \\alpha A B + \\beta C\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"base displaystyle textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")]),a("span",{staticClass:"mrel"},[t._v("←")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mord mathit"},[t._v("A")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")]),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")])])])])])]),t._v(" "),a("p",[t._v("计算的结果会把 C 的内存覆盖。")]),t._v(" "),a("p",[t._v("GEMM 对 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("β")]),a("mo",[t._v("=")]),a("mn",[t._v("0")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\beta=0")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mord mathrm"},[t._v("0")])])])]),t._v(" 有特殊处理，公式为 "),a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mi",[t._v("C")]),a("mo",[t._v("←")]),a("mi",[t._v("α")]),a("mi",[t._v("A")]),a("mi",[t._v("B")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("C \\leftarrow \\alpha A B")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),a("span",{staticClass:"base textstyle uncramped"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")]),a("span",{staticClass:"mrel"},[t._v("←")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")]),a("span",{staticClass:"mord mathit"},[t._v("A")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")])])])]),t._v("，即使 C 有浮点数吉祥三宝（即 inf, -inf, nan）也能正常运算。因此 C "),a("strong",[t._v("可以")]),t._v("是一块未初始化的内存。")]),t._v(" "),a("h2",{attrs:{id:"_2-参数"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-参数"}},[t._v("#")]),t._v(" 2. 参数")]),t._v(" "),a("p",[t._v("BLAS 有两套接口，CBLAS 和 BLAS (FORTRAN)。因为 torch 使用了 FORTRAN 接口，这里就按 FORTRAN 接口介绍。")]),t._v(" "),a("p",[t._v("GEMM 有 13 个参数：transa, transb, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc，含义如下：")]),t._v(" "),a("ul",[a("li",[t._v("transa, transb: 字符类型，控制矩阵 A, B 转置（字符 N 表示不转置 NO TRANSPOSE，T 表示转置，另外还支持 Hermitian 伴随，这个我们不关心）")]),t._v(" "),a("li",[t._v("m: 整数，A 的第一维")]),t._v(" "),a("li",[t._v("n: 整数，B 的第二维")]),t._v(" "),a("li",[t._v("k: 整数，公共维（A 的第二维、B 的第一维）")]),t._v(" "),a("li",[t._v("alpha, beta: 浮点数，即公式里的两个数字")]),t._v(" "),a("li",[t._v("a, b, c: 指针，指向矩阵 A, B, C 首地址（首行首列元素）")]),t._v(" "),a("li",[t._v("lda, ldb, ldc: 整数，是 A, B, C 矩阵的 stride，换个说法就是用 "),a("code",[t._v("a[i + j * lda]")]),t._v(" 访问矩阵 A 的 i 行 j 列")])]),t._v(" "),a("h2",{attrs:{id:"_3-行列优先顺序"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-行列优先顺序"}},[t._v("#")]),t._v(" 3. 行列优先顺序")]),t._v(" "),a("p",[t._v("这时候有人就会发现盲点，"),a("code",[t._v("a[i + j * lda]")]),t._v(" 的 lda 怎么乘到了 j 上。这是因为 BLAS 使用列优先顺序 (column-major order)。其实就是一列的元素连续储存在内存上，而不同行就需要用 lda 来索引。")]),t._v(" "),a("p",[t._v("在深度学习中使用的都是行优先顺序，所以需要做一些调整。")]),t._v(" "),a("blockquote",[a("p",[t._v("ps：如果用 CBLAS 接口可以直接指定行优先顺序 (row-major order)，不需要下面的奇怪操作了。")])]),t._v(" "),a("p",[t._v("显然，我们可以通过参数 transa, transb 来行列交换，这样矩阵 A, B 就不需要额外转置。")]),t._v(" "),a("p",[t._v("第二个问题，矩阵 C 也是列优先顺序，GEMM 不提供 transc 这样的参数，怎么办？答案是将 transa, transb 取反并交换 A, B，有公式：")]),t._v(" "),a("p",[a("span",{staticClass:"katex-display"},[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",[a("semantics",[a("mrow",[a("mstyle",{attrs:{scriptlevel:"0",displaystyle:"true"}},[a("msup",[a("mrow",[a("mo",{attrs:{fence:"true"}},[t._v("(")]),a("mi",[t._v("A")]),a("mi",[t._v("B")]),a("mo",{attrs:{fence:"true"}},[t._v(")")])],1),a("mrow",[a("mrow",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("T")])],1)],1)],1),a("mo",[t._v("=")]),a("msup",[a("mi",[t._v("B")]),a("mrow",[a("mrow",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("T")])],1)],1)],1),a("msup",[a("mi",[t._v("A")]),a("mrow",[a("mrow",[a("mi",{attrs:{mathvariant:"normal"}},[t._v("T")])],1)],1)],1)],1)],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\displaystyle \\left(AB\\right)^{\\mathrm {T} }=B^{\\mathrm {T} }A^{\\mathrm {T} }\n")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"strut",staticStyle:{height:"0.8913309999999999em"}}),a("span",{staticClass:"strut bottom",staticStyle:{height:"1.1413309999999999em","vertical-align":"-0.25em"}}),a("span",{staticClass:"base displaystyle textstyle uncramped"},[a("span",{staticClass:"reset-textstyle displaystyle textstyle uncramped"},[a("span",{staticClass:"minner"},[a("span",{staticClass:"minner displaystyle textstyle uncramped"},[a("span",{staticClass:"style-wrap reset-textstyle textstyle uncramped",staticStyle:{top:"0em"}},[t._v("(")]),a("span",{staticClass:"mord mathit"},[t._v("A")]),a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")]),a("span",{staticClass:"style-wrap reset-textstyle textstyle uncramped",staticStyle:{top:"0em"}},[t._v(")")])]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.413em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("T")])])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),a("span",{staticClass:"mrel"},[t._v("=")]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.413em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("T")])])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),a("span",{staticClass:"mord"},[a("span",{staticClass:"mord mathit"},[t._v("A")]),a("span",{staticClass:"vlist"},[a("span",{staticStyle:{top:"-0.413em","margin-right":"0.05em"}},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),a("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord scriptstyle uncramped"},[a("span",{staticClass:"mord mathrm"},[t._v("T")])])])])]),a("span",{staticClass:"baseline-fix"},[a("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[a("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])])]),t._v(" "),a("h2",{attrs:{id:"_4-数据类型"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-数据类型"}},[t._v("#")]),t._v(" 4. 数据类型")]),t._v(" "),a("p",[t._v("GEMM 支持 4 种数据类型：")]),t._v(" "),a("ol",[a("li",[t._v("sgemm: 单精度矩阵乘法 (s -> single)")]),t._v(" "),a("li",[t._v("dgemm: 双精度矩阵乘法 (d -> double)")]),t._v(" "),a("li",[t._v("cgemm: 单精度复数矩阵乘法")]),t._v(" "),a("li",[t._v("zgemm: 双精度复数矩阵乘法")])]),t._v(" "),a("p",[t._v("显然满足不了深度学习的需求，于是就有了：")]),t._v(" "),a("p",[t._v("onednn")]),t._v(" "),a("ul",[a("li",[t._v("gemm_bf16bf16f32: 输入 bfloat16 输出 float 的矩阵乘法")]),t._v(" "),a("li",[t._v("gemm_s8u8s32, gemm_s8s8s32: 整数的矩阵乘法")])]),t._v(" "),a("p",[t._v("cublas")]),t._v(" "),a("ul",[a("li",[t._v("cublasSgemmEx: 数据类型作为参数输入，可支持 bfloat16, half, int8 等类型。"),a("a",{attrs:{href:"https://docs.nvidia.com/cuda/cublas/#cublas-t-gemmex",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考"),a("OutboundLink")],1)])]),t._v(" "),a("h2",{attrs:{id:"_5-batch-扩展"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-batch-扩展"}},[t._v("#")]),t._v(" 5. batch 扩展")]),t._v(" "),a("p",[t._v("BMM (Batched Matrix Multiplication) 是深度学习常见的算子，它可以同时进行多个规格相同的 GEMM 运算。我们希望在 bmm 接口内完成更合理的多线程策略来提升性能。")]),t._v(" "),a("p",[t._v("例如，cublas 提供了三个接口：")]),t._v(" "),a("ol",[a("li",[t._v("cublasgemmBatched，参数 A, B, C 被换成指针数组 Aarray, Barray, Carray，通过 "),a("code",[t._v("Aarray[i]")]),t._v(" 得到第 i 个矩阵 A")]),t._v(" "),a("li",[t._v("cublasgemmStridedBatched，增加了 strideA, strideB, strideC，通过 "),a("code",[t._v("A + i * strideA")]),t._v(" 得到第 i 个矩阵 A")]),t._v(" "),a("li",[t._v("cublasgemmGroupedBatched，每个 GEMM 的规格也可以不一样")])]),t._v(" "),a("h2",{attrs:{id:"_6-pack-扩展"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-pack-扩展"}},[t._v("#")]),t._v(" 6. pack 扩展")]),t._v(" "),a("p",[t._v("一般来说 GEMM 会先对数据布局进行一些调整，又叫 pack。")]),t._v(" "),a("p",[t._v("pack 可以让真正 GEMM 计算时访存变得友好，从而提升性能。很显然，这一步也可以放在外面做，这样进行多次 GEMM 计算就不需要再 pack 了。onednn "),a("a",{attrs:{href:"https://www.intel.com/content/www/us/en/developer/articles/technical/introducing-the-new-packed-apis-for-gemm.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("例如：线性层 (linear) 的权重矩阵在推理时不会改变，那就不需要每次 pack 了。")]),t._v(" "),a("h2",{attrs:{id:"_7-算子融合"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-算子融合"}},[t._v("#")]),t._v(" 7. 算子融合")]),t._v(" "),a("p",[t._v("如果 GEMM 运算后面还要和 bias 向量做加法（如线性层中经常会出现 bias）。如果在 GEMM 里完成，就不需要反复读写矩阵 C 的内存。cublas "),a("a",{attrs:{href:"https://docs.nvidia.com/cuda/cublas/#cublasltepilogue-t",target:"_blank",rel:"noopener noreferrer"}},[t._v("参考"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("除了 bias，算子融合的方式有很多，例如压缩量化、激活函数等。")])])}),[],!1,null,null,null);a.default=i.exports}}]);